{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433d003d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac74d2ec-b8f0-40ca-b447-9f63cd513497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "import torchaudio\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from dit_clap import DiffusionTransformer\n",
    "from config.model.config import config\n",
    "from train.dataset import SampleDataset\n",
    "from condition.t5condition import T5Conditioner\n",
    "from train.validation import validate_or_test\n",
    "from train.utils import draw_plot, cleanup_memory, save_concatenated_mel_spectrogram, calculate_targets, prepare_batch_data, masked_loss\n",
    "from vae.get_function import create_autoencoder_from_config\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from inference.inference import generation\n",
    "from utils import get_span_mask, prob_mask_like\n",
    "from accelerate import Accelerator, DistributedDataParallelKwargs\n",
    "from safetensors.torch import load_file, save_file\n",
    "from cosine_annealing_warmup import CosineAnnealingWarmupRestarts\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "from train.dataset import Config\n",
    "from audiotools import AudioSignal\n",
    "from condition.clap_model import CLAPModule\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841af7cd-07f7-47cf-ac52-217962a3dc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "model = DiffusionTransformer(\n",
    "    d_model          = 1536,\n",
    "    depth            = 24,\n",
    "    num_heads        = 24,\n",
    "    input_concat_dim = 0,\n",
    "    global_cond_type = 'prepend',\n",
    "    latent_channels  = 64, \n",
    "    config           = config, \n",
    "    device           = device,\n",
    "    ada_cond_dim     = None,\n",
    "    use_skip         = False,\n",
    "    is_melody_prompt = True,\n",
    "    is_audio_prompt  = True\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "ae = create_autoencoder_from_config(config['auto_encoder_config']).to(device)\n",
    "text_conditioner = T5Conditioner(output_dim=768).to(device)\n",
    "\n",
    "ae.eval()\n",
    "text_conditioner.eval()\n",
    "model.eval()\n",
    "\n",
    "num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(num_trainable_params)\n",
    "\n",
    "model.load_state_dict(torch.load('/workspace/mel_con_sample/total_model_350.pth'), strict=False)\n",
    "# model.load_state_dict(torch.load('/workspace/mel_con_sample/main/weights_0205_clap_full/model_0.pth'))\n",
    "print(\"-\")\n",
    "\n",
    "ae_state_dict = torch.load('/workspace/mel_con_sample/vae_weight.pth')\n",
    "ae_clean_state_dict = {layer_name.replace('model.', ''): weights for layer_name, weights in ae_state_dict.items()}\n",
    "ae.load_state_dict(ae_clean_state_dict)\n",
    "\n",
    "del ae_clean_state_dict\n",
    "del ae_state_dict\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "clap_model = CLAPModule().to(device)\n",
    "clap_model.eval()\n",
    "\n",
    "print(\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1030e655-3f3b-4755-8f09-ae13448885ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from condition.voice import make_voice_cond\n",
    "\n",
    "cfg = {\n",
    "    \"sample_steps\": 100,\n",
    "    \"sample_cfg\": 6.5,\n",
    "    \"sample_rate\": 44100,\n",
    "    \"diffusion_objective\": \"v\"\n",
    "}\n",
    "\n",
    "audios = [\n",
    "    {\n",
    "        'audio_path': '/workspace/mel_con_sample/testsamples/bass_synth.wav',\n",
    "        'prompt': 'prompts: electric guitar, loop',\n",
    "        'duration': 3.7\n",
    "    },\n",
    "    {\n",
    "        'audio_path': '/workspace/mel_con_sample/testsamples/guitar_melody.wav',\n",
    "        'prompt': 'prompts: piano, loop',\n",
    "        'duration': 13.0\n",
    "    },\n",
    "    {\n",
    "        'audio_path': '/workspace/mel_con_sample/testsamples/hiphop_vocal.wav',\n",
    "        'prompt': 'prompts: hip-hop, vocal, loop, wet',\n",
    "        'duration': 6.9\n",
    "    },\n",
    "    {\n",
    "        'audio_path': '/workspace/mel_con_sample/testsamples/percussion.wav',\n",
    "        'prompt': 'prompts: electric drums, loop, wet',\n",
    "        'duration': 9.5\n",
    "    },\n",
    "    {\n",
    "        'audio_path': '/workspace/mel_con_sample/testsamples/acoustic_chords.wav',\n",
    "        'prompt': 'prompts: electric guitar, dry',\n",
    "        'duration': 4.1\n",
    "    },\n",
    "]\n",
    "\n",
    "for i, d in enumerate(audios):\n",
    "    ap = d['audio_path']\n",
    "    prompt = d['prompt']\n",
    "    duration = d['duration']\n",
    "    \n",
    "    audio, sr = librosa.load(ap, sr=44100)\n",
    "    audio = torch.tensor(audio)\n",
    "    audio = audio[:44100*10]\n",
    "    melody = make_voice_cond(audio)\n",
    "    zeros = torch.zeros((16, 215-melody.shape[-1]))\n",
    "    melody = torch.concat([melody, zeros], dim=-1)\n",
    "    melody = melody.unsqueeze(dim=0).to(device)\n",
    "    audio_embed = clap_model(audio.unsqueeze(dim=0))\n",
    "\n",
    "    audio_embed = torch.zeros_like(audio_embed)\n",
    "    # print('audio_embed ', audio_embed.shape)\n",
    "    # melody = torch.zeros((1, 16, 215)).to(device)\n",
    "    \n",
    "    output = generation(\n",
    "        model,\n",
    "        ae,\n",
    "        text_conditioner,\n",
    "        text=prompt,\n",
    "        steps=cfg['sample_steps'],\n",
    "        cfg_scale=cfg['sample_cfg'],\n",
    "        duration=duration,\n",
    "        sample_rate=cfg['sample_rate'],\n",
    "        batch_size=1,\n",
    "        device=device,\n",
    "        disable=False,\n",
    "        melody_prompt=melody,\n",
    "        audio_embed=audio_embed\n",
    "    )\n",
    "    # AudioSignal(ap).widget()\n",
    "    # print(output.shape)\n",
    "    # torchaudio.save(f'/workspace/generateds/generated_{i}.wav', output.cpu()[0], 44100)\n",
    "    AudioSignal(output.cpu().numpy()[0][:, :int(44100*duration)], sample_rate=44100).widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f34851-c4f8-4cb9-abbe-db04dd88a61a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf70cb9-0831-4299-929b-7d7fa689cd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed4a670-72c3-4d88-b4db-280f4e067720",
   "metadata": {},
   "outputs": [],
   "source": [
    "melody.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9877fb74-b247-4e68-9687-20e71bf9b1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/workspace/for_test_rank.csv\")\n",
    "\n",
    "df.iloc[0]['generated_prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616be6d7-3308-4ce3-bdf1-d97fb1bc341c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2c130d-f243-41fe-8bd6-567dc9cd03e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from condition.voice import make_voice_cond\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/workspace/for_test_rank.csv\")\n",
    "\n",
    "for i in range(10):\n",
    "    d = df.iloc[i]\n",
    "    \n",
    "    prompt = d['generated_prompt']\n",
    "    duration = d['duration']\n",
    "    \n",
    "    melody = torch.zeros((1, 16, 215)).to(device)\n",
    "    audio_embed = torch.zeros((1, 512)).to(device)\n",
    "    \n",
    "    cfg = {\n",
    "        \"sample_steps\": 100,\n",
    "        \"sample_cfg\": 7.0,\n",
    "        \"sample_duration\": duration,\n",
    "        \"sample_rate\": 44100,\n",
    "        \"diffusion_objective\": \"v\"\n",
    "    }\n",
    "\n",
    "    for j in range(2):\n",
    "        output = generation(\n",
    "             model,\n",
    "            ae,\n",
    "            text_conditioner,\n",
    "            text=prompt,\n",
    "            steps=cfg['sample_steps'],\n",
    "            cfg_scale=cfg['sample_cfg'],\n",
    "            duration=duration,\n",
    "            sample_rate=cfg['sample_rate'],\n",
    "            batch_size=1,\n",
    "            device=device,\n",
    "            disable=False,\n",
    "            melody_prompt=melody,\n",
    "            audio_embed=audio_embed\n",
    "        )\n",
    "        # torchaudio.save(f'/workspace/generateds/generated_{i}_{j}.wav', output.cpu()[0], 44100)\n",
    "        AudioSignal(output.cpu().numpy()[0][:, :int(44100*duration)], sample_rate=44100).widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd1c2c6-598f-4f62-80ab-36f8f2587186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1282f48-6088-4bd3-a3b8-336d0978ddaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from condition.voice import make_voice_cond\n",
    "\n",
    "cfg = {\n",
    "    \"sample_steps\": 100,\n",
    "    \"sample_cfg\": 6.0,\n",
    "    \"sample_rate\": 44100,\n",
    "    \"diffusion_objective\": \"v\"\n",
    "}\n",
    "seed = 12\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "audios = [\n",
    "    {\n",
    "        'audio_path': './FMT_-_Bm_-_Analog_Phazer_Chords.wav',\n",
    "        'prompt': 'prompts: guitar, blues, rock, electric, funk, live sounds, high, loop, key: b major, bpm: 105',\n",
    "        # 'prompt': 'prompts: guitar, blues, loop',\n",
    "        'duration': 4.5\n",
    "    }\n",
    "]\n",
    "\n",
    "# for i, d in enumerate(audios):\n",
    "for i, d in enumerate(audios[:1]):\n",
    "    inst_ap = audios[0]['audio_path']\n",
    "    melody_ap = audios[0]['audio_path']\n",
    "    prompt = audios[0]['prompt']\n",
    "    duration = audios[0]['duration']\n",
    "    \n",
    "    audio, sr = librosa.load(melody_ap, sr=44100)\n",
    "    audio = torch.tensor(audio)\n",
    "    audio = audio[:44100*10]\n",
    "    audio = audio\n",
    "    melody = make_voice_cond(audio)\n",
    "    zeros = torch.zeros((16, 215-melody.shape[-1]))\n",
    "    melody = torch.concat([melody, zeros], dim=-1)\n",
    "    melody = melody.unsqueeze(dim=0).to(device)\n",
    "\n",
    "    # # melody = torch.where(melody >= 0.5, melody, torch.tensor(0.0))\n",
    "    \n",
    "    # audio, sr = librosa.load(inst_ap, sr=44100)\n",
    "    # audio = torch.tensor(audio)\n",
    "    # audio = audio[:44100*10]\n",
    "    # audio_embed = clap_model(audio.unsqueeze(dim=0))\n",
    "    \n",
    "    # melody = torch.zeros((1, 16, 215)).to(device)\n",
    "    # audio_embed = torch.zeros((1, 512)).to(device)\n",
    "    \n",
    "    output = generation(\n",
    "        model,\n",
    "        ae,\n",
    "        text_conditioner,\n",
    "        text=prompt,\n",
    "        steps=cfg['sample_steps'],\n",
    "        cfg_scale=cfg['sample_cfg'],\n",
    "        duration=duration,\n",
    "        sample_rate=cfg['sample_rate'],\n",
    "        batch_size=1, \n",
    "        device=device,\n",
    "        disable=False,\n",
    "        melody_prompt=melody,\n",
    "        audio_embed=audio_embed\n",
    "    )\n",
    "    AudioSignal(output.cpu().numpy()[:, :int(44100*duration)], sample_rate=44100).widget()\n",
    "    # AudioSignal(inst_ap).widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a59349-d315-4055-a448-4f0c56bc550a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a286feb6-c081-48db-89e4-fe368917a0ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9858b4e-036c-44b0-a1cd-5eb341a816ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_chromagram(chromagram, sample_rate):\n",
    "    \"\"\"\n",
    "    시각화 함수\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # chromagram을 NumPy 배열로 변환\n",
    "    chroma_np = chromagram.squeeze().cpu().detach().numpy()  # 텐서를 NumPy로 변환\n",
    "    # 시각화\n",
    "    plt.imshow(chroma_np, aspect='auto', origin='lower', cmap='coolwarm', interpolation='nearest')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.xlabel('Frames')\n",
    "    plt.ylabel('Chroma Bins')\n",
    "    plt.title('Chromagram')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d9bcaa-9136-40d2-8c31-305e30aded70",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_chromagram(melody, sample_rate=44100)\n",
    "\n",
    "melody = make_voice_cond(output[0][:, :int(44100*duration)].cpu().detach())\n",
    "zeros = torch.zeros((16, 215-melody.shape[-1]))\n",
    "melody = torch.concat([melody, zeros], dim=-1)\n",
    "visualize_chromagram(melody, sample_rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82617a2-ff4e-4fa7-8f46-e5ac5d26a47e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7ed5cf-7420-496f-9d9e-ef7fe0aaeea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import torchaudio\n",
    "\n",
    "ap = './FMT_-_Bm_-_Analog_Phazer_Chords.wav'\n",
    "\n",
    "audio, sr = librosa.load(ap, sr=44100)\n",
    "audio = torch.tensor(audio)\n",
    "audio = torch.stack([audio, audio], dim=0).to(device)\n",
    "print(audio.shape, sr)\n",
    "codes = ae.encode(audio)\n",
    "print(codes.shape)\n",
    "\n",
    "decodes = ae.decode(codes)\n",
    "print(decodes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5589fc2-933a-44ef-84bb-2e2114bf00b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "AudioSignal(ap).widget()\n",
    "AudioSignal(decodes.cpu().numpy().squeeze(), sample_rate=44100).widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4615ce50-d18a-4da0-91f9-66b76ba7be73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7a85c2-a52f-49fc-bd39-c4262b3c54f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35719d6f-86c9-4ef2-8411-b9f9d33468ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
